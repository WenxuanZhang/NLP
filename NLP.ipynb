{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing \n",
    "**Project Summary**: This project intended to explore what makes an online post popular. Here are the steps of this project.\n",
    "1. Web Scraping and Download All the articles.\n",
    "2. Detect Topics using LDA model\n",
    "3. Try various Machine Learning Algorithms to predict the view of post "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib2\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "online_news = pd.read_csv(\"/Users/wenxuanzhang/Local Doc/Documents/Work/UM/OnlineNewsPopularity.csv\")\n",
    "test = online_news['url'][1]\n",
    "r = requests.get(test)\n",
    "soup = BeautifulSoup(r.content,\"html.parser\")\n",
    "online_news['title'] = \"\"\n",
    "online_news[\"content\"] =\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are 39644 articles,which takes more than 330 hours to collect,parse and clean, we are only going to take first 600 hundred articles to LDA analaysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wenxuanzhang/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/wenxuanzhang/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# using data to estimate the standard time \n",
    "start = timeit.timeit()\n",
    "\n",
    "for i in range(0,600):\n",
    "    article = requests.get(online_news['url'][i])\n",
    "    soup = BeautifulSoup(article.content,\"html.parser\")\n",
    "    a = \" \".join([x.get_text() for x in soup.find_all('p')])\n",
    "    title = \" \".join([x.get_text() for x in soup.find_all('p')])\n",
    "    full = a + title \n",
    "    online_news['title'][i] = title\n",
    "    online_news['content'][i] = full\n",
    "    \n",
    "end = timeit.timeit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the articles as local documents thus we could avoid web scraping the data online again and again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,600):\n",
    "    filename = str(i)+'.txt'\n",
    "    text_file = open(filename, \"w\")\n",
    "    with open(filename, \"w\") as text_file:\n",
    "        text_file.write(online_news['content'][i].encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(art).split()]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ladmodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.029*\"ap\" + 0.029*\"tweet\" + 0.029*\"twitter\" + 0.018*\"way\" + 0.018*\"medium\" + 0.018*\"news\" + 0.018*\"samsung\"'), (1, u'0.009*\"operation\" + 0.009*\"statement\" + 0.009*\"international\" + 0.009*\"la\" + 0.009*\"money\" + 0.009*\"spent\" + 0.009*\"clear\"'), (2, u'0.009*\"breakingnewscom\" + 0.009*\"step\" + 0.009*\"breaking\" + 0.009*\"compromising\" + 0.009*\"consumer\" + 0.009*\"account\" + 0.009*\"editor\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=3, num_words=7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_lda=ladmodel[doc_term_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IDEA : Using the Tom_lib model to analyze the best number of topics https://pypi.python.org/pypi/tom_lib/0.2.2\n",
    "# IDEA 2: Using https://de.dariah.eu/tatom/topic_model_python.html To generate the TOPIC Distance\n",
    "# IDEAR 3: Training Machine Learing model to predict the popularity of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tom_lib as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in file in to list "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
