{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing \n",
    "**Project Summary**: This project intended to explore what makes an online post popular. Here are the steps of this project.\n",
    "1. Web Scraping and Download All the articles.\n",
    "2. Detect Topics using LDA model\n",
    "3. Try various Machine Learning Algorithms to predict the view of post "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib2\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "online_news = pd.read_csv(\"/Users/wenxuanzhang/Local Doc/Documents/Work/UM/OnlineNewsPopularity.csv\")\n",
    "test = online_news['url'][1]\n",
    "r = requests.get(test)\n",
    "soup = BeautifulSoup(r.content,\"html.parser\")\n",
    "online_news['title'] = \"\"\n",
    "online_news[\"content\"] =\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are 39644 articles,which takes more than 330 hours to collect,parse and clean, we are only going to take first 600 hundred articles to LDA analaysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wenxuanzhang/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/wenxuanzhang/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# using data to estimate the standard time \n",
    "start = timeit.timeit()\n",
    "\n",
    "for i in range(0,600):\n",
    "    article = requests.get(online_news['url'][i])\n",
    "    soup = BeautifulSoup(article.content,\"html.parser\")\n",
    "    a = \" \".join([x.get_text() for x in soup.find_all('p')])\n",
    "    title = \" \".join([x.get_text() for x in soup.find_all('p')])\n",
    "    full = a + title \n",
    "    online_news['title'][i] = title\n",
    "    online_news['content'][i] = full\n",
    "    \n",
    "end = timeit.timeit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the articles as local documents thus we could avoid web scraping the data online again and again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,600):\n",
    "    filename = str(i)+'.txt'\n",
    "    text_file = open(filename, \"w\")\n",
    "    with open(filename, \"w\") as text_file:\n",
    "        text_file.write(online_news['content'][i].encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(art).split()]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ladmodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.029*\"ap\" + 0.029*\"tweet\" + 0.029*\"twitter\" + 0.018*\"way\" + 0.018*\"medium\" + 0.018*\"news\" + 0.018*\"samsung\"'), (1, u'0.009*\"operation\" + 0.009*\"statement\" + 0.009*\"international\" + 0.009*\"la\" + 0.009*\"money\" + 0.009*\"spent\" + 0.009*\"clear\"'), (2, u'0.009*\"breakingnewscom\" + 0.009*\"step\" + 0.009*\"breaking\" + 0.009*\"compromising\" + 0.009*\"consumer\" + 0.009*\"account\" + 0.009*\"editor\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=3, num_words=7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_lda=ladmodel[doc_term_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IDEA : Using the Tom_lib model to analyze the best number of topics https://pypi.python.org/pypi/tom_lib/0.2.2\n",
    "# IDEA 2: Using https://de.dariah.eu/tatom/topic_model_python.html To generate the TOPIC Distance\n",
    "# IDEAR 3: Training Machine Learing model to predict the popularity of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path ='/Users/wenxuanzhang/LocalDoc/GitHub/NLP/files/*.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in file in to list \n",
    "Doc = []\n",
    "files = glob.glob(path)\n",
    "# iterate over the list getting each file \n",
    "for i in range(0,len(files)):\n",
    "    content = open(files[i]) \n",
    "    Doc = Doc + [content.read().decode('unicode_escape').encode('ascii','ignore')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "from stop_words import get_stop_words\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "from gensim import corpora, models\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through document list\n",
    "texts = []\n",
    "for i in Doc:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.025*\"s\" + 0.018*\"read\" + 0.015*\"can\" + 0.011*\"technolog\"'), (1, u'0.017*\"s\" + 0.006*\"imag\" + 0.006*\"will\" + 0.006*\"courtesi\"')]\n"
     ]
    }
   ],
   "source": [
    "# Select the number of topics in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.025*\"s\" + 0.018*\"read\" + 0.015*\"can\" + 0.011*\"technolog\" + 0.010*\"editor\" + 0.009*\"mashabl\" + 0.009*\"report\" + 0.008*\"new\" + 0.007*\"game\" + 0.007*\"t\"'),\n",
       " (1,\n",
       "  u'0.017*\"s\" + 0.006*\"imag\" + 0.006*\"will\" + 0.006*\"courtesi\" + 0.005*\"can\" + 0.005*\"photo\" + 0.005*\"also\" + 0.005*\"one\" + 0.005*\"use\" + 0.005*\"like\"')]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
