{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing \n",
    "**Project Summary**: This project intended to explore what makes an online post popular. Here are the steps of this project.\n",
    "1. Web Scraping and Download All the articles.\n",
    "2. Detect Topics using LDA model\n",
    "3. Try various Machine Learning Algorithms to predict the view of post "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "online_news = pd.read_csv(\"/Users/wenxuanzhang/Local Doc/Documents/Work/UM/OnlineNewsPopularity.csv\")\n",
    "test = online_news['url'][1]\n",
    "r = requests.get(test)\n",
    "soup = BeautifulSoup(r.content,\"html.parser\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "online_news[\"url\"][1]\n",
    "type(r)\n",
    "a =  soup.find_all('p')\n",
    "  \n",
    "text = a[1].get_text()\n",
    "text2 = a[2].get_text()\n",
    "art = \" \".join([x.get_text() for x in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Mashable', u\"AP's Twitter to Begin Displaying Sponsored Tweets\"]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.get_text()for x in soup.find_all(\"h1\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(art).split()]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ladmodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.029*\"ap\" + 0.029*\"tweet\" + 0.029*\"twitter\" + 0.018*\"way\" + 0.018*\"medium\" + 0.018*\"news\" + 0.018*\"samsung\"'), (1, u'0.009*\"operation\" + 0.009*\"statement\" + 0.009*\"international\" + 0.009*\"la\" + 0.009*\"money\" + 0.009*\"spent\" + 0.009*\"clear\"'), (2, u'0.009*\"breakingnewscom\" + 0.009*\"step\" + 0.009*\"breaking\" + 0.009*\"compromising\" + 0.009*\"consumer\" + 0.009*\"account\" + 0.009*\"editor\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=3, num_words=7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lda=ladmodel[doc_term_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tom_lib\n",
      "  Downloading tom_lib-0.2.2.tar.gz (819kB)\n",
      "\u001b[K    100% |████████████████████████████████| 829kB 761kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /Users/wenxuanzhang/anaconda/lib/python2.7/site-packages (from tom_lib)\n",
      "Requirement already satisfied: networkx in /Users/wenxuanzhang/anaconda/lib/python2.7/site-packages (from tom_lib)\n",
      "Requirement already satisfied: pandas in /Users/wenxuanzhang/anaconda/lib/python2.7/site-packages (from tom_lib)\n",
      "Requirement already satisfied: scipy in /Users/wenxuanzhang/anaconda/lib/python2.7/site-packages (from tom_lib)\n",
      "Requirement already satisfied: numpy in /Users/wenxuanzhang/anaconda/lib/python2.7/site-packages (from tom_lib)\n",
      "Requirement already satisfied: decorator>=3.4.0 in /Users/wenxuanzhang/anaconda/lib/python2.7/site-packages (from networkx->tom_lib)\n",
      "Requirement already satisfied: python-dateutil in /Users/wenxuanzhang/anaconda/lib/python2.7/site-packages (from pandas->tom_lib)\n",
      "Requirement already satisfied: pytz>=2011k in /Users/wenxuanzhang/anaconda/lib/python2.7/site-packages (from pandas->tom_lib)\n",
      "Requirement already satisfied: six>=1.5 in /Users/wenxuanzhang/anaconda/lib/python2.7/site-packages (from python-dateutil->pandas->tom_lib)\n",
      "Building wheels for collected packages: tom-lib\n",
      "  Running setup.py bdist_wheel for tom-lib ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/wenxuanzhang/Library/Caches/pip/wheels/de/e5/91/94b42068e3f5a0471264a2d496241e5e642882ab25aaaa5adc\n",
      "Successfully built tom-lib\n",
      "Installing collected packages: tom-lib\n",
      "Successfully installed tom-lib-0.2.2\n"
     ]
    }
   ],
   "source": [
    "! pip install tom_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IDEA : Using the Tom_lib model to analyze the best number of topics https://pypi.python.org/pypi/tom_lib/0.2.2\n",
    "# IDEA 2: Using https://de.dariah.eu/tatom/topic_model_python.html To generate the TOPIC Distance\n",
    "# IDEAR 3: Training Machine Learing model to predict the popularity of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
