Behind all the dazzling mobile-ready electronics products on display at the International CES in Las Vegas this week is a looming problem: how to make the networks that support all these wireless devices function robustly and efficiently.  With less fanfare than you’d see in Vegas, potential solutions are arising in labs in like Pittsburgh, Los Angeles, and New Brunswick, New Jersey. The grand challenge is to overhaul the Internet to better serve an expected flood of 15 billion network-connected devices by 2015—many of them mobile—up from five billion today, according to Intel estimates. The Internet was designed in the 1960s to dispatch data to fixed addresses of static PCs connected to a single network, but today it connects a riot of diverse gadgets that can zip from place to place and connect to many different networks. As the underlying networks have been reworked and added-to to make way for new technologies, some serious inefficiencies and security problems have arisen. "Nobody really expects the network to crash when you add one more device," says Peter Steenkiste, computer scientist at Carnegie Mellon University. "But I do have a sense this is more of a creeping problem of complexity." Over the past year, fundamentally new network designs have taken shape and are being tested at universities around the United States under the National Science Foundation's Future Internet Architectures Project, launched in 2010. One key idea is that networks should be able to obtain data from the nearest location — not seek it from some specific data center at a fixed address. "Today I have on my desk a smartphone, a tablet and a Mac computer. To move data between them, today the request goes all the way to the cloud — God knows where that is — so it can come back here to another device that is two feet away," says Lixia Zhang, a computer scientist at the University of California, Los Angeles. "That is wrong, it is simply wrong." Things would work quite differently under the Named Data Networking (NDN) project that Zhang heads. Under NDN, data packets are assigned addresses that emphasize the information they contain — not just the IP address of where they came from or where they are going. These codes could, among other things, allow easy sharing of data directly between devices.  "In the end, I think we can improve the speed, throughput, and overall efficiency. Today you have many data centers that can have thousands of people asking for same piece of data. In the NDN system, you just find the nearest copy of that data," says Zhang. "Conceptually, this is pretty simple, but it is really a revolution." This data-centric concept allows security and privacy settings to be cryptographically attached directly to the data—with different settings depending on how sensitive the data is—rather than relying on measures such as VPNs and firewalls. In addition to Zhang’s project, the NSF effort also funds Internet architecture projects with similar goals at Rutgers, the University of Pennsylvania, and CMU, where Steenkiste runs the expressive Internet architecture, or XIA, project. David Clark, the MIT computer scientist and the Internet’s former chief protocol architect, says it’s too early to say which will win out. “All are research, all are speculative, and are potentially exciting,” he says, but he adds that the NDN effort “is the most revolutionary—the project really changes the underlying model of what a network does. It replaces communication among end-points with access to data, wherever it may be.” Several early demonstrations of these new Internet architecture projects have taken place over the past year, and more are expected in 2013. It is early days for these efforts, says Dipankar Raychaudhuri, head of the Rutgers Winlab, who runs its NSF-funded Mobility First project, which tries to make mobile devices and car networks a more seamless part of the infrastructure. Still, he predicts that “in another two years, you should be able to see comparative evaluations and metrics,” that show the value of the projects. Among other things, new architectures could allow devices to attach to two or more networks at the same time. Today your smartphone can switch back and forth between, say, 4G and Wi-Fi, but not use them both and combine the data coming from each. The root of the problem is that the original protocols assumed only a single network interface. “You could, in principle, remain connected to both networks, and the network could decide how to send you the data at each moment,” Raychaudhuri says. Meanwhile, some existing applications help fill a gap. For example, the network optimization company Akamai — which runs 119,000 servers and delivers between 15 and 30% of the web’s traffic — has for several years been offering something called Net Session. This application allows device-to-device file transfers, rather than server-to-device downloads, and is popular in developing countries where connectivity is poor. So far, Net Session has been installed on 30 million devices, most of them laptops. “The goal is to expand it so it can support mobile handsets, tablets, and media-type boxes at home,” says Kris Alexander, director of strategy at Akamai.  However, making that leap is no small feat. No Net Session app for iOS or Android is available, and the main reason is that the necessary processing power and battery drain is too large. Photo courtesy of TechCocktail 
This article originally published at MIT Technology Review
here
Behind all the dazzling mobile-ready electronics products on display at the International CES in Las Vegas this week is a looming problem: how to make the networks that support all these wireless devices function robustly and efficiently.  With less fanfare than you’d see in Vegas, potential solutions are arising in labs in like Pittsburgh, Los Angeles, and New Brunswick, New Jersey. The grand challenge is to overhaul the Internet to better serve an expected flood of 15 billion network-connected devices by 2015—many of them mobile—up from five billion today, according to Intel estimates. The Internet was designed in the 1960s to dispatch data to fixed addresses of static PCs connected to a single network, but today it connects a riot of diverse gadgets that can zip from place to place and connect to many different networks. As the underlying networks have been reworked and added-to to make way for new technologies, some serious inefficiencies and security problems have arisen. "Nobody really expects the network to crash when you add one more device," says Peter Steenkiste, computer scientist at Carnegie Mellon University. "But I do have a sense this is more of a creeping problem of complexity." Over the past year, fundamentally new network designs have taken shape and are being tested at universities around the United States under the National Science Foundation's Future Internet Architectures Project, launched in 2010. One key idea is that networks should be able to obtain data from the nearest location — not seek it from some specific data center at a fixed address. "Today I have on my desk a smartphone, a tablet and a Mac computer. To move data between them, today the request goes all the way to the cloud — God knows where that is — so it can come back here to another device that is two feet away," says Lixia Zhang, a computer scientist at the University of California, Los Angeles. "That is wrong, it is simply wrong." Things would work quite differently under the Named Data Networking (NDN) project that Zhang heads. Under NDN, data packets are assigned addresses that emphasize the information they contain — not just the IP address of where they came from or where they are going. These codes could, among other things, allow easy sharing of data directly between devices.  "In the end, I think we can improve the speed, throughput, and overall efficiency. Today you have many data centers that can have thousands of people asking for same piece of data. In the NDN system, you just find the nearest copy of that data," says Zhang. "Conceptually, this is pretty simple, but it is really a revolution." This data-centric concept allows security and privacy settings to be cryptographically attached directly to the data—with different settings depending on how sensitive the data is—rather than relying on measures such as VPNs and firewalls. In addition to Zhang’s project, the NSF effort also funds Internet architecture projects with similar goals at Rutgers, the University of Pennsylvania, and CMU, where Steenkiste runs the expressive Internet architecture, or XIA, project. David Clark, the MIT computer scientist and the Internet’s former chief protocol architect, says it’s too early to say which will win out. “All are research, all are speculative, and are potentially exciting,” he says, but he adds that the NDN effort “is the most revolutionary—the project really changes the underlying model of what a network does. It replaces communication among end-points with access to data, wherever it may be.” Several early demonstrations of these new Internet architecture projects have taken place over the past year, and more are expected in 2013. It is early days for these efforts, says Dipankar Raychaudhuri, head of the Rutgers Winlab, who runs its NSF-funded Mobility First project, which tries to make mobile devices and car networks a more seamless part of the infrastructure. Still, he predicts that “in another two years, you should be able to see comparative evaluations and metrics,” that show the value of the projects. Among other things, new architectures could allow devices to attach to two or more networks at the same time. Today your smartphone can switch back and forth between, say, 4G and Wi-Fi, but not use them both and combine the data coming from each. The root of the problem is that the original protocols assumed only a single network interface. “You could, in principle, remain connected to both networks, and the network could decide how to send you the data at each moment,” Raychaudhuri says. Meanwhile, some existing applications help fill a gap. For example, the network optimization company Akamai — which runs 119,000 servers and delivers between 15 and 30% of the web’s traffic — has for several years been offering something called Net Session. This application allows device-to-device file transfers, rather than server-to-device downloads, and is popular in developing countries where connectivity is poor. So far, Net Session has been installed on 30 million devices, most of them laptops. “The goal is to expand it so it can support mobile handsets, tablets, and media-type boxes at home,” says Kris Alexander, director of strategy at Akamai.  However, making that leap is no small feat. No Net Session app for iOS or Android is available, and the main reason is that the necessary processing power and battery drain is too large. Photo courtesy of TechCocktail 
This article originally published at MIT Technology Review
here
